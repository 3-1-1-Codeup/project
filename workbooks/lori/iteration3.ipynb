{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrangle as w\n",
    "import model as m\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New wrangle to test out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "def get_311_data():\n",
    "    '''\n",
    "    This function uses pandas read .csv to read in the downloaded .csv \n",
    "    from: https://data.sanantonio.gov/dataset/service-calls/resource/20eb6d22-7eac-425a-85c1-fdb365fd3cd7\n",
    "    after the .csv is read in, it returns it as a data frame.\n",
    "    '''\n",
    "    df= pd.read_csv('service_calls.csv')\n",
    "    return df\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Set the index\n",
    "def drop_and_index(df):\n",
    "    \"\"\"\n",
    "    This function will take in one positional argurment:\n",
    "    1.  311 df\n",
    "    This function will perform the following operations to the df:\n",
    "    1.  Drop category, drop report starting date, and drop report\n",
    "    ending date\n",
    "    2.  Set CASEID as the index\n",
    "    \"\"\"\n",
    "    # Drop category, report starting date, and report ending date\n",
    "    df.drop(columns=['Category', \n",
    "                 'Report Starting Date', \n",
    "                 'Report Ending Date' ], inplace=True)\n",
    "    # Set index to case id\n",
    "    df.set_index('CASEID', inplace=True)\n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Handle null values\n",
    "def handle_nulls(df):\n",
    "    '''This funciton takes in data frame\n",
    "    drops nulls for specified features\n",
    "    replaces nulls with \"Unknown\" for specific feature\n",
    "    removes observations where City Council is the Department'''\n",
    "    # drop null values\n",
    "    df.dropna(subset = ['SLA_Date', 'XCOORD', 'YCOORD'], inplace = True)\n",
    "    # replace null values\n",
    "    df.fillna({'Dept': 'Unknown'}, inplace = True)\n",
    "    # remove city council department\n",
    "    df = df[df.Dept != 'City Council']\n",
    "    # return df\n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Create delay columns\n",
    "def create_delay_columns(df):\n",
    "    '''This funciton takes in the dataframe\n",
    "    reformats specified columns into datetime format\n",
    "    create 2 columns to see time a case was open \n",
    "    and the time it was given for a resolution to be found\n",
    "    create another feature for how long it took \n",
    "        compare to due date for a resolution to be found\n",
    "    bin how long it took compared to due date\n",
    "    fill nulls with \"Still Open\"\n",
    "    return df\n",
    "    '''\n",
    "    # make sure the open and closed date columns are formatted in datetime format\n",
    "    df['OPENEDDATETIME'] = pd.to_datetime(df['OPENEDDATETIME'])\n",
    "    df['CLOSEDDATETIME'] = pd.to_datetime(df['CLOSEDDATETIME'])\n",
    "    df['SLA_Date'] = pd.to_datetime(df['SLA_Date'])\n",
    "    # create new number of days open feature\n",
    "    df['days_open'] = df['CLOSEDDATETIME'] - df['OPENEDDATETIME']\n",
    "    # Create new column to hold days before needed resolution\n",
    "    df['resolution_days_due'] = df['SLA_Date'] - df['OPENEDDATETIME']\n",
    "    # Convert to string format insted of timedelta\n",
    "    df['days_open'] = df.days_open // pd.Timedelta('1d')\n",
    "    # Convert to string format insted of timedelta\n",
    "    df['resolution_days_due'] = df.resolution_days_due // pd.Timedelta('1d')\n",
    "    # create new feature to show how long it took to resolve compared to resolution due date\n",
    "    df['days_before_or_after_due'] = df.resolution_days_due - df.days_open\n",
    "    # replace null values in days open with 0\n",
    "    df['days_open'] = df['days_open'].fillna(0)\n",
    "    # add 1 to resolution days to offset future issues with upcoming feature\n",
    "    df['resolution_days_due'] = df['resolution_days_due'] + 1\n",
    "    # create new feature to show how long it took to resolve compared to resolution due date\n",
    "    df['pct_time_of_used'] = df.days_open / df.resolution_days_due\n",
    "    # bin the new feature\n",
    "    df['level_of_delay'] = pd.cut(df.pct_time_of_used, \n",
    "                            bins = [-20.0,0.5,0.75,1.0,15,800],\n",
    "                            labels = ['Very Early Response', \n",
    "                                      'Early Response', \"On Time Response\", \"Late Response\", \n",
    "                                      'Very Late Response'])\n",
    "    # drop nulls in these columns\n",
    "    df.dropna(subset=['days_open'], how='all', inplace=True)\n",
    "    df.dropna(subset=['level_of_delay'], how='all', inplace=True)\n",
    "    # return df\n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "def handle_outliers(df):\n",
    "    '''removes outiers from df'''\n",
    "    # remove outliers from days_open\n",
    "    df = df[df.days_open < 1400]\n",
    "    # return df\n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "def create_dummies(df):\n",
    "    '''This function creates dummy variables for Council Districts'''\n",
    "    # Drop district 0\n",
    "    df = df[df['Council District'] != 0]\n",
    "    # set what we are going to create these dummies from\n",
    "    dummy_df =  pd.get_dummies(df['Council District'])\n",
    "    # Name the new columns\n",
    "    dummy_df.columns = ['district_1', 'district_2', \n",
    "                        'district_3', 'district_4', 'district_5',\n",
    "                        'district_6', 'district_7', 'district_8',\n",
    "                        'district_9', 'district_10']\n",
    "    # add the dummies to the data frame\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "    return df\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "def clean_reason(df):\n",
    "    '''\n",
    "    This function will take in the service call df and replace the content of REASONNAME column with condensed names\n",
    "    '''\n",
    "    # replace with waste\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Waste Collection', 'Solid Waste', 'Brush'], 'waste')\n",
    "    # replace with code\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Code Enforcement', 'Code Enforcement (Internal)', 'Code Enforcement (IntExp)'], 'code')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Field Operations', 'Vector'], 'field')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace('Miscellaneous', 'misc')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Traffic Operations', 'Traffic Engineering Design', 'Traffic Issue Investigation'], 'traffic')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Streets', 'Signals', 'Signs and Markings'], 'streets')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace('Trades', 'trades')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Stormwater', 'Storm Water', 'Storm Water Engineering'], 'storm')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Small Business', 'Food Establishments', 'Shops (Internal)', 'Shops'], 'business')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace('Workforce Development', 'workforce')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Customer Service', '311 Call Center', 'Director\\'s Office Horizontal'], 'customer_service')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Land Development', 'Clean and Green', 'Urban Forestry', 'Natural Resources', 'Park Projects', 'Tree Crew', 'District 2', 'Clean and Green Natural Areas'], 'land')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace('Facility License', 'license')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Dangerous Premise','Historic Preservation', 'Engineering Division'], 'buildings')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Graffiti (IntExp)', 'General Sanitation', 'Graffiti'], 'cleanup')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Waste Collection', 'Solid Waste'], 'waste')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Waste Collection', 'Solid Waste'], 'waste')\n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# rename columns\n",
    "def clean_column_names(df):\n",
    "    '''This function reads in a dataframe as a positional argument, makes the column names easier to call and\n",
    "    more python friendly. It also extracts the zip code from the address column. It then returns a cleaned data \n",
    "    frame.'''\n",
    "    df= df.rename(columns={\n",
    "                    'Category':'category', 'OPENEDDATETIME':'open_date', 'Dept': 'dept',\n",
    "                    'SLA_Date':'due_date', 'CLOSEDDATETIME': 'closed_date', 'Late (Yes/No)': 'is_late',\n",
    "                    'OBJECTDESC': 'address', 'REASONNAME': 'call_reason', 'TYPENAME': 'case_type', \n",
    "                    'Council District': 'council_district', 'CASEID': 'case_id',\n",
    "                    'CaseStatus': 'case_status', 'SourceID':'source_id', 'XCOORD': 'longitude', 'YCOORD': 'latitude',\n",
    "                    'Report Starting Date': 'report_start_date', 'Report Ending Date': 'report_end_date'\n",
    "                      })\n",
    "    df['zipcode'] = df['address'].str.extract(r'.*(\\d{5}?)$')  \n",
    "    #drop zipcode nulls after obtaining zipcode\n",
    "    df.dropna(subset=['zipcode'], how='all', inplace=True)         \n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# clean the whole df\n",
    "def first_iteration_clean_311(df):\n",
    "    '''Takes in all previous funcitons to clean the whole df'''\n",
    "    # Drop columns and set index\n",
    "    df = drop_and_index(df)\n",
    "    # hadle null values\n",
    "    df = handle_nulls(df)\n",
    "    # creating delay involved columns\n",
    "    df = create_delay_columns(df)\n",
    "    # handle outliers\n",
    "    df = handle_outliers(df)\n",
    "    # make dummies\n",
    "    df = create_dummies(df)\n",
    "    # merge reasons\n",
    "    df = clean_reason(df)\n",
    "    # rename columns\n",
    "    df = clean_column_names(df)\n",
    "    df.to_csv('first_iteration_clean_311.csv')\n",
    "    # return df\n",
    "    return df\n",
    "\n",
    "\n",
    "# Train/Split the data~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "def split(df, stratify_by= 'level_of_delay'):\n",
    "    \"\"\"\n",
    "    Crude train, validate, test split\n",
    "    To stratify, send in a column name\n",
    "    \"\"\"\n",
    "    if stratify_by == None:\n",
    "        train, test = train_test_split(df, test_size=.2, random_state=319)\n",
    "        train, validate = train_test_split(train, test_size=.3, random_state=319)\n",
    "    else:\n",
    "        train, test = train_test_split(df, test_size=.2, random_state=319, stratify=df[stratify_by])\n",
    "        train, validate = train_test_split(train, test_size=.3, random_state=319, stratify=train[stratify_by])\n",
    "    return train, validate, test\n",
    "\n",
    "# Create X_train, y_train, etc...~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "def separate_y(train, validate, test):\n",
    "    '''\n",
    "    This function will take the train, validate, and test dataframes and separate the target variable into its\n",
    "    own panda series\n",
    "    '''\n",
    "    \n",
    "    X_train = train.drop(columns=['level_of_delay'])\n",
    "    y_train = train.level_of_delay\n",
    "    X_validate = validate.drop(columns=['level_of_delay'])\n",
    "    y_validate = validate.level_of_delay\n",
    "    X_test = test.drop(columns=['level_of_delay'])\n",
    "    y_test = test.level_of_delay\n",
    "    return X_train, y_train, X_validate, y_validate, X_test, y_test\n",
    "\n",
    "# Scale the data~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "def scale_data(X_train, X_validate, X_test):\n",
    "    '''\n",
    "    This function will scale numeric data using Min Max transform after \n",
    "    it has already been split into train, validate, and test.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    obj_col = ['open_date', 'due_date', 'closed_date', 'is_late', 'dept', 'call_reason', 'case_type', 'case_status', 'source_id', 'address', 'zipcode']\n",
    "    num_train = X_train.drop(columns = obj_col)\n",
    "    num_validate = X_validate.drop(columns = obj_col)\n",
    "    num_test = X_test.drop(columns = obj_col)\n",
    "    \n",
    "    \n",
    "    # Make the thing\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    \n",
    "   \n",
    "    # we only .fit on the training data\n",
    "    scaler.fit(num_train)\n",
    "    train_scaled = scaler.transform(num_train)\n",
    "    validate_scaled = scaler.transform(num_validate)\n",
    "    test_scaled = scaler.transform(num_test)\n",
    "    \n",
    "    # turn the numpy arrays into dataframes\n",
    "    train_scaled = pd.DataFrame(train_scaled, columns=num_train.columns)\n",
    "    validate_scaled = pd.DataFrame(validate_scaled, columns=num_train.columns)\n",
    "    test_scaled = pd.DataFrame(test_scaled, columns=num_train.columns)\n",
    "    \n",
    "    \n",
    "    return train_scaled, validate_scaled, test_scaled\n",
    "\n",
    "# Combo Train & Scale Function~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "def split_separate_scale(df, stratify_by= 'level_of_delay'):\n",
    "    '''\n",
    "    This function will take in a dataframe\n",
    "    separate the dataframe into train, validate, and test dataframes\n",
    "    separate the target variable from train, validate and test\n",
    "    then it will scale the numeric variables in train, validate, and test\n",
    "    finally it will return all dataframes individually\n",
    "    '''\n",
    "    \n",
    "    # split data into train, validate, test\n",
    "    train, validate, test = split(df, stratify_by= 'level_of_delay')\n",
    "    \n",
    "     # seperate target variable\n",
    "    X_train, y_train, X_validate, y_validate, X_test, y_test = separate_y(train, validate, test)\n",
    "    \n",
    "    \n",
    "    # scale numeric variable\n",
    "    train_scaled, validate_scaled, test_scaled = scale_data(X_train, X_validate, X_test)\n",
    "    \n",
    "    return train, validate, test, X_train, y_train, X_validate, y_validate, X_test, y_test, train_scaled, validate_scaled, test_scaled\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def get_sq_miles(df):\n",
    "    \"\"\"\n",
    "    This function will apply the square miles per district\n",
    "    to each district.\n",
    "    \"\"\"\n",
    "    # Creating a dictionary with square miles from city of San Antonio to convert to dataframe\n",
    "    sq_miles = {1: 26.00, 2: 59.81, 3: 116.15, 4: 65.21, 5: 22.24, 6: 38.44, 7: 32.82,\n",
    "                         8: 71.64, 9: 48.71, 10: 55.62}\n",
    "    # Converting to dataframe\n",
    "    sq_miles = pd.DataFrame(list(sq_miles.items()),columns = ['council_district','square_miles'])\n",
    "    #Merging with the original dataframe\n",
    "    df = df.merge(sq_miles, on = 'council_district', how ='left')\n",
    "    return df\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def extract_time(df):\n",
    "    '''\n",
    "    This function will take in a dataframe and return it with new features extracted from the open_date column\n",
    "    - open_month: which month the case was opened in\n",
    "    - open_year: which year the case was opened in\n",
    "    - open_week: which week the case was opened in\n",
    "    '''\n",
    "    \n",
    "    # extract month from open_date\n",
    "    df['open_month'] = df.open_date.dt.month\n",
    "    \n",
    "    # extract year from open_date\n",
    "    df['open_year'] = df.open_date.dt.year\n",
    "    \n",
    "    # extract week from open_date\n",
    "    df['open_week'] = df.open_date.dt.week\n",
    "    \n",
    "    return df\n",
    "    \n",
    "#------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def find_voter_info(df):\n",
    "    '''This function reads in a dataframe. Using the Council District column, it appends the voter turn out and\n",
    "    number of registered voters for each district. It does NOT take into account district 0 due to that being a\n",
    "    filler district for outside jurisdictions. It then appends the info onto the dataframe and returns it for later use.'''\n",
    "    conditions = [\n",
    "    (df['Council District'] == 1),\n",
    "    (df['Council District'] == 2), \n",
    "    (df['Council District'] == 3),\n",
    "    (df['Council District'] == 4),\n",
    "    (df['Council District'] == 5),\n",
    "    (df['Council District'] == 6),\n",
    "    (df['Council District'] == 7),\n",
    "    (df['Council District'] == 8),\n",
    "    (df['Council District'] == 9),\n",
    "    (df['Council District'] == 10)\n",
    "    ]\n",
    "    # create a list of the values we want to assign for each condition\n",
    "    voter_turnout = [0.148, 0.086, 0.111, 0.078, 0.085, 0.124, 0.154, 0.137, 0.185, 0.154]\n",
    "    registered_voters= [68081, 67656, 69022, 66370, 61418, 80007, 83287, 97717, 99309, 91378]\n",
    "    # create a new column and use np.select to assign values to it using our lists as arguments\n",
    "    df['voter_turnout_2019'] = np.select(conditions, voter_turnout)\n",
    "    df['num_of_registered_voters'] = np.select(conditions, registered_voters)\n",
    "    return df\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def add_per_cap_in(df):\n",
    "    '''\n",
    "    This function takes in the original cleaned dataframe and adds a per capita\n",
    "    income metric by council district\n",
    "    '''\n",
    "    # Creating a dictionary with per_capita values from city of San Antonio to convert to dataframe\n",
    "    per_cap_in = {1: 23967, 2: 19055, 3: 18281, 4: 18500, 5: 13836, 6: 23437, 7: 25263,\n",
    "                         8: 35475, 9: 42559, 10: 30240}\n",
    "    # Converting to dataframe\n",
    "    per_cap_in = pd.DataFrame(list(per_cap_in.items()),columns = ['council_district','per_capita_income'])\n",
    "    #Merging with the original dataframe\n",
    "    df = df.merge(per_cap_in, on = 'council_district', how ='left')\n",
    "    return df\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# clean the whole df\n",
    "# clean the whole df\n",
    "def clean_311(df):\n",
    "    '''Takes in all previous functions to clean the whole df'''\n",
    "    # Drop columns and set index\n",
    "    df = drop_and_index(df)\n",
    "    # hadle null values\n",
    "    df = handle_nulls(df)\n",
    "    # creating delay involved columns\n",
    "    df = create_delay_columns(df)\n",
    "    # handle outliers\n",
    "    df = handle_outliers(df)\n",
    "    # make dummies\n",
    "    df = create_dummies(df)\n",
    "    # merge reasons\n",
    "    df = clean_reason(df)\n",
    "    #add voter information\n",
    "    df= find_voter_info(df)\n",
    "    # rename columns\n",
    "    df = clean_column_names(df)\n",
    "    # add date/time information\n",
    "    df= extract_time(df)\n",
    "    #add per capita information\n",
    "    df= add_per_cap_in(df)\n",
    "    #add per sqmiles info\n",
    "    df = get_sq_miles(df)\n",
    "    #remove extra nulls\n",
    "    df.dropna(subset = ['days_before_or_after_due', 'closed_date'], inplace = True)\n",
    "    #make clean csv with all changes\n",
    "    df.to_csv('second_clean_311.csv')\n",
    "    # return df\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New model with zip dummz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wrangle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Model Prep\n",
    "\n",
    "\n",
    "def dummy_dept(df):\n",
    "    # dummy dept feature\n",
    "    dummy_df =  pd.get_dummies(df['dept'])\n",
    "    # Name the new columns\n",
    "    dummy_df.columns = ['animal_care_services', 'code_enforcement_services', \n",
    "                        'customer_services', 'development_services', \n",
    "                        'metro_health', 'parks_and_rec',\n",
    "                        'solid_waste_management', 'trans_and_cap_improvements', \n",
    "                        'unknown_dept']\n",
    "    # add the dummies to the data frame\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "    return df\n",
    "#-----------------------------------------------------------------------------    \n",
    "def dummy_call_reason(df):\n",
    "    # dummy dept feature\n",
    "    dummy_df =  pd.get_dummies(df['call_reason'])\n",
    "    # Name the new columns\n",
    "    dummy_df.columns = ['buildings', 'business', 'cleanup', 'code',\n",
    "                        'customer_service', 'field', 'land',\n",
    "                        'license', 'misc', 'storm', 'streets', 'trades', \n",
    "                        'traffic', 'waste']\n",
    "    # add the dummies to the data frame\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "    return df\n",
    "#-----------------------------------------------------------------------------\n",
    "def make_source_id_dummies(df):\n",
    "    '''This function takes in the cleaned dataframe, makes dummy variables of the source id column, readds the names of the\n",
    "    dummy columns and returns the concatenated dummy dataframe to the original dataframe.'''\n",
    "    #make dummies\n",
    "    dummy_df = pd.get_dummies(df['source_id'])\n",
    "    #add back column names\n",
    "    dummy_df.columns = ['web_portal', '311_mobile_app', 'constituent_call', 'internal_services_requests']\n",
    "    # concatenate dummies to the cleaned data frame\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "    return df\n",
    "\n",
    "#-------------------------------\n",
    "#zipcode dummies\n",
    "def dummy_zipcodes(df):\n",
    "    dummy = pd.get_dummies(df['zipcode'])\n",
    "    df = pd.concat([df, dummy], axis=1)\n",
    "    return df\n",
    "\n",
    "#-------------------------------\n",
    "def keep_info(df):\n",
    "    df.drop(df.columns.difference(['dept','call_reason', 'source_id', 'level_of_delay',\n",
    "                                   'council_district', 'resolution_days_due', 'district_0', 'district_1', 'district_2',\n",
    "                                   'district_3', 'district_4','district_5', 'district_6', 'district_7', 'district_8', \n",
    "                                   'district_9','district_10', 'per_capita_income', 'zipcode']), 1, inplace=True)\n",
    "    return df\n",
    "\n",
    "#--------------------------------\n",
    "def model_df():\n",
    "    '''This function reads in the clean 311 dataframe, applies all of the above functions to prepare it for modeling. \n",
    "    The function then returns a cleaned dataframe ready for modeling.'''\n",
    "    df= clean_311(get_311_data())\n",
    "    df= keep_info(df)\n",
    "    df= dummy_dept(df)\n",
    "    df= dummy_call_reason(df)\n",
    "    df= make_source_id_dummies(df)\n",
    "    df= dummy_zipcodes(df)\n",
    "\n",
    "    return df\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def split(df, stratify_by= 'level_of_delay'):\n",
    "    \"\"\"\n",
    "    Crude train, validate, test split\n",
    "    To stratify, send in a column name\n",
    "    \"\"\"\n",
    "    if stratify_by == None:\n",
    "        train, test = train_test_split(df, test_size=.2, random_state=319)\n",
    "        train, validate = train_test_split(train, test_size=.3, random_state=319)\n",
    "    else:\n",
    "        train, test = train_test_split(df, test_size=.2, random_state=319, stratify=df[stratify_by])\n",
    "        train, validate = train_test_split(train, test_size=.3, random_state=319, stratify=train[stratify_by])\n",
    "    return train, validate, test\n",
    "\n",
    "\n",
    "#------------------------------------\n",
    "def separate_y(train, validate, test):\n",
    "    '''\n",
    "    This function will take the train, validate, and test dataframes and separate the target variable into its\n",
    "    own panda series\n",
    "    '''\n",
    "    \n",
    "    X_train = train.drop(columns=['level_of_delay'])\n",
    "    y_train = train.level_of_delay\n",
    "    X_validate = validate.drop(columns=['level_of_delay'])\n",
    "    y_validate = validate.level_of_delay\n",
    "    X_test = test.drop(columns=['level_of_delay'])\n",
    "    y_test = test.level_of_delay\n",
    "    return X_train, y_train, X_validate, y_validate, X_test, y_test\n",
    "#------------------------------------\n",
    "def scale_data(X_train, X_validate, X_test):\n",
    "    '''\n",
    "    This function will scale numeric data using Min Max transform after \n",
    "    it has already been split into train, validate, and test.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    obj_col = []\n",
    "    num_train = X_train.drop(columns = obj_col)\n",
    "    num_validate = X_validate.drop(columns = obj_col)\n",
    "    num_test = X_test.drop(columns = obj_col)\n",
    "    \n",
    "    \n",
    "    # Make the thing\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    \n",
    "   \n",
    "    # we only .fit on the training data\n",
    "    scaler.fit(num_train)\n",
    "    train_scaled = scaler.transform(num_train)\n",
    "    validate_scaled = scaler.transform(num_validate)\n",
    "    test_scaled = scaler.transform(num_test)\n",
    "    \n",
    "    # turn the numpy arrays into dataframes\n",
    "    train_scaled = pd.DataFrame(train_scaled, columns=num_train.columns)\n",
    "    validate_scaled = pd.DataFrame(validate_scaled, columns=num_train.columns)\n",
    "    test_scaled = pd.DataFrame(test_scaled, columns=num_train.columns)\n",
    "    \n",
    "    \n",
    "    return train_scaled, validate_scaled, test_scaled\n",
    "\n",
    "#------------------------------------\n",
    "\n",
    "def split_separate_scale(df, stratify_by= 'level_of_delay'):\n",
    "    '''\n",
    "    This function will take in a dataframe\n",
    "    separate the dataframe into train, validate, and test dataframes\n",
    "    separate the target variable from train, validate and test\n",
    "    then it will scale the numeric variables in train, validate, and test\n",
    "    finally it will return all dataframes individually\n",
    "    '''\n",
    "    \n",
    "    # split data into train, validate, test\n",
    "    train, validate, test = split(df, stratify_by= 'level_of_delay')\n",
    "    \n",
    "     # seperate target variable\n",
    "    X_train, y_train, X_validate, y_validate, X_test, y_test = separate_y(train, validate, test)\n",
    "    \n",
    "    \n",
    "    # scale numeric variable\n",
    "    train_scaled, validate_scaled, test_scaled = scale_data(X_train, X_validate, X_test)\n",
    "    \n",
    "    return train, validate, test, X_train, y_train, X_validate, y_validate, X_test, y_test, train_scaled, validate_scaled, test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_311(get_311_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = model_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['dept', 'call_reason', \n",
    "        'source_id', 'council_district', 'zipcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train, validate, test, X_train, y_train, X_validate, y_validate, X_test, y_test, train_scaled, validate_scaled, test_scaled = split_separate_scale(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline\n",
    "train.level_of_delay.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline accuracy will be early response\n",
    "baseline = round((train.level_of_delay == 'Very Early Response').mean(), 2) *100\n",
    "\n",
    "print(f'The baseline accuracy is: {baseline} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the thing\n",
    "clf = DecisionTreeClassifier(max_depth=3, random_state=123)\n",
    "#fit the thing\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#predicitons\n",
    "y_pred = clf.predict(X_train)\n",
    "#probability\n",
    "y_pred_proba = clf.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the accuracy \n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "      .format(clf.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of Decision Tree classifier on validate set: {:.2f}'\n",
    "     .format(clf.score(X_validate, y_validate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = ['resolution_days_due',\n",
    " 'district_1',\n",
    " 'district_2',\n",
    " 'district_3',\n",
    " 'district_4',\n",
    " 'district_5',\n",
    " 'district_6',\n",
    " 'district_7',\n",
    " 'district_8',\n",
    " 'district_9',\n",
    " 'district_10',\n",
    " 'per_capita_income',\n",
    " 'animal_care_services',\n",
    " 'code_enforcement_services',\n",
    " 'customer_services',\n",
    " 'development_services',\n",
    " 'metro_health',\n",
    " 'parks_and_rec',\n",
    " 'solid_waste_management',\n",
    " 'trans_and_cap_improvements',\n",
    " 'unknown_dept',\n",
    " 'buildings',\n",
    " 'business',\n",
    " 'cleanup',\n",
    " 'code',\n",
    " 'customer_service',\n",
    " 'field',\n",
    " 'land',\n",
    " 'license',\n",
    " 'misc',\n",
    " 'storm',\n",
    " 'streets',\n",
    " 'trades',\n",
    " 'traffic',\n",
    " 'waste',\n",
    " 'web_portal',\n",
    " '311_mobile_app',\n",
    " 'constituent_call',\n",
    " 'internal_services_requests',\n",
    " '78015',\n",
    " '78023',\n",
    " '78073',\n",
    " '78109',\n",
    " '78112',\n",
    " '78154',\n",
    " '78201',\n",
    " '78202',\n",
    " '78203',\n",
    " '78204',\n",
    " '78205',\n",
    " '78207',\n",
    " '78208',\n",
    " '78209',\n",
    " '78210',\n",
    " '78211',\n",
    " '78212',\n",
    " '78213',\n",
    " '78214',\n",
    " '78215',\n",
    " '78216',\n",
    " '78217',\n",
    " '78218',\n",
    " '78219',\n",
    " '78220',\n",
    " '78221',\n",
    " '78222',\n",
    " '78223',\n",
    " '78224',\n",
    " '78225',\n",
    " '78226',\n",
    " '78227',\n",
    " '78228',\n",
    " '78229',\n",
    " '78230',\n",
    " '78231',\n",
    " '78232',\n",
    " '78233',\n",
    " '78234',\n",
    " '78235',\n",
    " '78236',\n",
    " '78237',\n",
    " '78238',\n",
    " '78239',\n",
    " '78240',\n",
    " '78242',\n",
    " '78244',\n",
    " '78245',\n",
    " '78247',\n",
    " '78248',\n",
    " '78249',\n",
    " '78250',\n",
    " '78251',\n",
    " '78252',\n",
    " '78253',\n",
    " '78254',\n",
    " '78255',\n",
    " '78256',\n",
    " '78257',\n",
    " '78258',\n",
    " '78259',\n",
    " '78260',\n",
    " '78261',\n",
    " '78264',\n",
    " '78266',\n",
    " '78284',\n",
    " '78288']\n",
    "#weights = ['uniform', 'density']\n",
    "knn = KNeighborsClassifier(n_neighbors=10, weights='uniform')\n",
    "#fit model\n",
    "knn.fit(X_train[features1], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mkae predictions\n",
    "y_pred_knn = knn.predict(X_train[features1])\n",
    "#estimate probability\n",
    "y_pred_proba = knn.predict_proba(X_train[features1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy\n",
    "accuracy = knn.score(X_train[features1], y_train)\n",
    "print(f\"KNN Accuracy is {accuracy:.5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the thing\n",
    "clf = DecisionTreeClassifier(max_depth=6, random_state=123)\n",
    "#fit the thing\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#predicitons\n",
    "y_pred = clf.predict(X_train)\n",
    "#probability\n",
    "y_pred_proba = clf.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the accuracy \n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "      .format(clf.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of Decision Tree classifier on validate set: {:.2f}'\n",
    "     .format(clf.score(X_validate, y_validate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN zip only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features2 = ['78015',\n",
    " '78023',\n",
    " '78073',\n",
    " '78109',\n",
    " '78112',\n",
    " '78154',\n",
    " '78201',\n",
    " '78202',\n",
    " '78203',\n",
    " '78204',\n",
    " '78205',\n",
    " '78207',\n",
    " '78208',\n",
    " '78209',\n",
    " '78210',\n",
    " '78211',\n",
    " '78212',\n",
    " '78213',\n",
    " '78214',\n",
    " '78215',\n",
    " '78216',\n",
    " '78217',\n",
    " '78218',\n",
    " '78219',\n",
    " '78220',\n",
    " '78221',\n",
    " '78222',\n",
    " '78223',\n",
    " '78224',\n",
    " '78225',\n",
    " '78226',\n",
    " '78227',\n",
    " '78228',\n",
    " '78229',\n",
    " '78230',\n",
    " '78231',\n",
    " '78232',\n",
    " '78233',\n",
    " '78234',\n",
    " '78235',\n",
    " '78236',\n",
    " '78237',\n",
    " '78238',\n",
    " '78239',\n",
    " '78240',\n",
    " '78242',\n",
    " '78244',\n",
    " '78245',\n",
    " '78247',\n",
    " '78248',\n",
    " '78249',\n",
    " '78250',\n",
    " '78251',\n",
    " '78252',\n",
    " '78253',\n",
    " '78254',\n",
    " '78255',\n",
    " '78256',\n",
    " '78257',\n",
    " '78258',\n",
    " '78259',\n",
    " '78260',\n",
    " '78261',\n",
    " '78264',\n",
    " '78266',\n",
    " '78284',\n",
    " '78288']\n",
    "\n",
    "#weights = ['uniform', 'density']\n",
    "knn = KNeighborsClassifier(n_neighbors=10, weights='uniform')\n",
    "#fit model\n",
    "knn.fit(X_train[features2], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mkae predictions\n",
    "y_pred_knn = knn.predict(X_train[features2])\n",
    "#estimate probability\n",
    "y_pred_proba = knn.predict_proba(X_train[features2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy\n",
    "accuracy = knn.score(X_train[features2], y_train)\n",
    "print(f\"KNN Accuracy is {accuracy:.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
