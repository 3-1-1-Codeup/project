{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrangle as w\n",
    "import model as m\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(495440, 13)\n",
      "drop and index (481205, 13)\n",
      "before: (481205, 18)\n",
      "after: (481205, 18)\n",
      "OPENEDDATETIME                   0\n",
      "SLA_Date                         0\n",
      "CLOSEDDATETIME               49509\n",
      "Late (Yes/No)                    0\n",
      "Dept                             0\n",
      "REASONNAME                       0\n",
      "TYPENAME                         0\n",
      "CaseStatus                       0\n",
      "SourceID                         0\n",
      "OBJECTDESC                       0\n",
      "Council District                 0\n",
      "XCOORD                           0\n",
      "YCOORD                           0\n",
      "days_open                        0\n",
      "resolution_days_due              0\n",
      "days_before_or_after_due     49509\n",
      "pct_time_of_used                 0\n",
      "level_of_delay              157845\n",
      "dtype: int64\n",
      "after level of delay drop (323360, 18)\n",
      "OPENEDDATETIME              0\n",
      "SLA_Date                    0\n",
      "CLOSEDDATETIME              0\n",
      "Late (Yes/No)               0\n",
      "Dept                        0\n",
      "REASONNAME                  0\n",
      "TYPENAME                    0\n",
      "CaseStatus                  0\n",
      "SourceID                    0\n",
      "OBJECTDESC                  0\n",
      "Council District            0\n",
      "XCOORD                      0\n",
      "YCOORD                      0\n",
      "days_open                   0\n",
      "resolution_days_due         0\n",
      "days_before_or_after_due    0\n",
      "pct_time_of_used            0\n",
      "level_of_delay              0\n",
      "dtype: int64\n",
      "create delay (323360, 18)\n",
      "outliers (322845, 18)\n",
      "dummies (321723, 28)\n",
      "clean reason (321723, 28)\n",
      "find voter (321723, 30)\n",
      "clean column names (278554, 31)\n",
      "extract times (278554, 34)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-8e1d5dc1548b>:335: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['open_week'] = df.open_date.dt.week\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add per cap (278554, 35)\n",
      "square miles (278554, 36)\n"
     ]
    }
   ],
   "source": [
    "df = clean_311(get_311_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('311_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_311(df):\n",
    "    '''Takes in all previous funcitons to clean the whole df'''\n",
    "    # Drop columns and set index\n",
    "    df = drop_and_index(df)\n",
    "    print(df.shape)\n",
    "    # hadle null values\n",
    "    df = handle_nulls(df)\n",
    "    print('drop and index', df.shape)\n",
    "    # creating delay involved columns\n",
    "    df = create_delay_columns(df)\n",
    "    print('create delay', df.shape)\n",
    "    # handle outliers\n",
    "    df = handle_outliers(df)\n",
    "    print('outliers', df.shape)\n",
    "    # make dummies\n",
    "    df = create_dummies(df)\n",
    "    print('dummies', df.shape)\n",
    "    # merge reasons\n",
    "    df = clean_reason(df)\n",
    "    print('clean reason', df.shape)\n",
    "    #add voter information\n",
    "    df= find_voter_info(df)\n",
    "    print('find voter', df.shape)\n",
    "    # rename columns\n",
    "    df = clean_column_names(df)\n",
    "    print('clean column names', df.shape)\n",
    "    # add date/time information\n",
    "    df= extract_time(df)\n",
    "    print('extract times', df.shape)\n",
    "    #add per capita information\n",
    "    df= add_per_cap_in(df)\n",
    "    print('add per cap', df.shape)\n",
    "    #add per sqmiles info\n",
    "    df = get_sq_miles(df)\n",
    "    print('square miles', df.shape)\n",
    "    #make clean csv with all changes\n",
    "    # df.to_csv('second_clean_311.csv')\n",
    "    # return df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "def get_311_data():\n",
    "    '''\n",
    "    This function uses pandas read .csv to read in the downloaded .csv \n",
    "    from: https://data.sanantonio.gov/dataset/service-calls/resource/20eb6d22-7eac-425a-85c1-fdb365fd3cd7\n",
    "    after the .csv is read in, it returns it as a data frame.\n",
    "    '''\n",
    "    df= pd.read_csv('service_calls.csv')\n",
    "    return df\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Set the index\n",
    "def drop_and_index(df):\n",
    "    \"\"\"\n",
    "    This function will take in one positional argurment:\n",
    "    1.  311 df\n",
    "    This function will perform the following operations to the df:\n",
    "    1.  Drop category, drop report starting date, and drop report\n",
    "    ending date\n",
    "    2.  Set CASEID as the index\n",
    "    \"\"\"\n",
    "    # Drop category, report starting date, and report ending date\n",
    "    df.drop(columns=['Category', \n",
    "                 'Report Starting Date', \n",
    "                 'Report Ending Date' ], inplace=True)\n",
    "    # Set index to case id\n",
    "    df.set_index('CASEID', inplace=True)\n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Handle null values\n",
    "def handle_nulls(df):\n",
    "    '''This funciton takes in data frame\n",
    "    drops nulls for specified features\n",
    "    replaces nulls with \"Unknown\" for specific feature\n",
    "    removes observations where City Council is the Department'''\n",
    "    # drop null values\n",
    "    df.dropna(subset = ['SLA_Date', 'XCOORD', 'YCOORD'], inplace = True)\n",
    "    # replace null values\n",
    "    df.fillna({'Dept': 'Unknown'}, inplace = True)\n",
    "    # remove city council department\n",
    "    df = df[df.Dept != 'City Council']\n",
    "    # return df\n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Create delay columns\n",
    "def create_delay_columns(df):\n",
    "    '''This funciton takes in the dataframe\n",
    "    reformats specified columns into datetime format\n",
    "    create 2 columns to see time a case was open \n",
    "    and the time it was given for a resolution to be found\n",
    "    create another feature for how long it took \n",
    "        compare to due date for a resolution to be found\n",
    "    bin how long it took compared to due date\n",
    "    fill nulls with \"Still Open\"\n",
    "    return df\n",
    "    '''\n",
    "    # make sure the open and closed date columns are formatted in datetime format\n",
    "    df['OPENEDDATETIME'] = pd.to_datetime(df['OPENEDDATETIME'])\n",
    "    df['CLOSEDDATETIME'] = pd.to_datetime(df['CLOSEDDATETIME'])\n",
    "    df['SLA_Date'] = pd.to_datetime(df['SLA_Date'])\n",
    "    # create new number of days open feature\n",
    "    df['days_open'] = df['CLOSEDDATETIME'] - df['OPENEDDATETIME']\n",
    "    # Create new column to hold days before needed resolution\n",
    "    df['resolution_days_due'] = df['SLA_Date'] - df['OPENEDDATETIME']\n",
    "    # Convert to string format insted of timedelta\n",
    "    df['days_open'] = df.days_open // pd.Timedelta('1d')\n",
    "    # Convert to string format insted of timedelta\n",
    "    df['resolution_days_due'] = df.resolution_days_due // pd.Timedelta('1d')\n",
    "    # create new feature to show how long it took to resolve compared to resolution due date\n",
    "    df['days_before_or_after_due'] = df.resolution_days_due - df.days_open\n",
    "    # replace null values in days open with 0\n",
    "    df['days_open'] = df['days_open'].fillna(0)\n",
    "    # add 1 to resolution days to offset future issues with upcoming feature\n",
    "    df['resolution_days_due'] = df['resolution_days_due'] + 1\n",
    "    # create new feature to show how long it took to resolve compared to resolution due date\n",
    "    df['pct_time_of_used'] = df.days_open / df.resolution_days_due\n",
    "    # bin the new feature\n",
    "    df['level_of_delay'] = pd.cut(df.pct_time_of_used, \n",
    "                            bins = [0.0,0.25,0.5,0.75,1.0,15,100,200],\n",
    "                            labels = ['Extremely Early Response', 'Very Early Response', \n",
    "                                      'Early Response', \"On Time Response\", \"Late Response\", \n",
    "                                      'Very Late Response', 'Extremely Late Response'])\n",
    "    # drop nulls in these columns\n",
    "    print('before:', df.shape)\n",
    "    df.dropna(subset=['days_open'], how='all', inplace=True)\n",
    "    print('after:', df.shape)\n",
    "    print(df.isna().sum())\n",
    "    df.dropna(subset=['level_of_delay'], how='all', inplace=True)\n",
    "    print('after level of delay drop', df.shape)\n",
    "    print(df.isna().sum())\n",
    "    # return new df\n",
    "    return df\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "def handle_outliers(df):\n",
    "    '''removes outiers from df'''\n",
    "    # remove outliers from days_open\n",
    "    df = df[df.days_open < 1400]\n",
    "    # return df\n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "def create_dummies(df):\n",
    "    '''This function creates dummy variables for Council Districts'''\n",
    "    # Drop district 0\n",
    "    df = df[df['Council District'] != 0]\n",
    "    # set what we are going to create these dummies from\n",
    "    dummy_df =  pd.get_dummies(df['Council District'])\n",
    "    # Name the new columns\n",
    "    dummy_df.columns = ['district_1', 'district_2', \n",
    "                        'district_3', 'district_4', 'district_5',\n",
    "                        'district_6', 'district_7', 'district_8',\n",
    "                        'district_9', 'district_10']\n",
    "    # add the dummies to the data frame\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "    return df\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "def clean_reason(df):\n",
    "    '''\n",
    "    This function will take in the service call df and replace the content of REASONNAME column with condensed names\n",
    "    '''\n",
    "    # replace with waste\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Waste Collection', 'Solid Waste', 'Brush'], 'waste')\n",
    "    # replace with code\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Code Enforcement', 'Code Enforcement (Internal)', 'Code Enforcement (IntExp)'], 'code')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Field Operations', 'Vector'], 'field')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace('Miscellaneous', 'misc')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Traffic Operations', 'Traffic Engineering Design', 'Traffic Issue Investigation'], 'traffic')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Streets', 'Signals', 'Signs and Markings'], 'streets')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace('Trades', 'trades')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Stormwater', 'Storm Water', 'Storm Water Engineering'], 'storm')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Small Business', 'Food Establishments', 'Shops (Internal)', 'Shops'], 'business')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace('Workforce Development', 'workforce')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Customer Service', '311 Call Center', 'Director\\'s Office Horizontal'], 'customer_service')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Land Development', 'Clean and Green', 'Urban Forestry', 'Natural Resources', 'Park Projects', 'Tree Crew', 'District 2', 'Clean and Green Natural Areas'], 'land')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace('Facility License', 'license')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Dangerous Premise','Historic Preservation', 'Engineering Division'], 'buildings')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Graffiti (IntExp)', 'General Sanitation', 'Graffiti'], 'cleanup')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Waste Collection', 'Solid Waste'], 'waste')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Waste Collection', 'Solid Waste'], 'waste')\n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# rename columns\n",
    "def clean_column_names(df):\n",
    "    '''This function reads in a dataframe as a positional argument, makes the column names easier to call and\n",
    "    more python friendly. It also extracts the zip code from the address column. It then returns a cleaned data \n",
    "    frame.'''\n",
    "    df= df.rename(columns={\n",
    "                    'Category':'category', 'OPENEDDATETIME':'open_date', 'Dept': 'dept',\n",
    "                    'SLA_Date':'due_date', 'CLOSEDDATETIME': 'closed_date', 'Late (Yes/No)': 'is_late',\n",
    "                    'OBJECTDESC': 'address', 'REASONNAME': 'call_reason', 'TYPENAME': 'case_type', \n",
    "                    'Council District': 'council_district', 'CASEID': 'case_id',\n",
    "                    'CaseStatus': 'case_status', 'SourceID':'source_id', 'XCOORD': 'longitude', 'YCOORD': 'latitude',\n",
    "                    'Report Starting Date': 'report_start_date', 'Report Ending Date': 'report_end_date'\n",
    "                      })\n",
    "    df['zipcode'] = df['address'].str.extract(r'.*(\\d{5}?)$')  \n",
    "    #drop zipcode nulls after obtaining zipcode\n",
    "    df.dropna(subset=['zipcode'], how='all', inplace=True)         \n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# clean the whole df\n",
    "def first_iteration_clean_311(df):\n",
    "    '''Takes in all previous funcitons to clean the whole df'''\n",
    "    # Drop columns and set index\n",
    "    df = drop_and_index(df)\n",
    "    # hadle null values\n",
    "    df = handle_nulls(df)\n",
    "    # creating delay involved columns\n",
    "    df = create_delay_columns(df)\n",
    "    # handle outliers\n",
    "    df = handle_outliers(df)\n",
    "    # make dummies\n",
    "    df = create_dummies(df)\n",
    "    # merge reasons\n",
    "    df = clean_reason(df)\n",
    "    # rename columns\n",
    "    df = clean_column_names(df)\n",
    "    df.to_csv('first_iteration_clean_311.csv')\n",
    "    # return df\n",
    "    return df\n",
    "\n",
    "\n",
    "# Train/Split the data~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "def split(df, stratify_by= 'level_of_delay'):\n",
    "    \"\"\"\n",
    "    Crude train, validate, test split\n",
    "    To stratify, send in a column name\n",
    "    \"\"\"\n",
    "    if stratify_by == None:\n",
    "        train, test = train_test_split(df, test_size=.2, random_state=319)\n",
    "        train, validate = train_test_split(train, test_size=.3, random_state=319)\n",
    "    else:\n",
    "        train, test = train_test_split(df, test_size=.2, random_state=319, stratify=df[stratify_by])\n",
    "        train, validate = train_test_split(train, test_size=.3, random_state=319, stratify=train[stratify_by])\n",
    "    return train, validate, test\n",
    "\n",
    "# Create X_train, y_train, etc...~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "def separate_y(train, validate, test):\n",
    "    '''\n",
    "    This function will take the train, validate, and test dataframes and separate the target variable into its\n",
    "    own panda series\n",
    "    '''\n",
    "    \n",
    "    X_train = train.drop(columns=['level_of_delay'])\n",
    "    y_train = train.level_of_delay\n",
    "    X_validate = validate.drop(columns=['level_of_delay'])\n",
    "    y_validate = validate.level_of_delay\n",
    "    X_test = test.drop(columns=['level_of_delay'])\n",
    "    y_test = test.level_of_delay\n",
    "    return X_train, y_train, X_validate, y_validate, X_test, y_test\n",
    "\n",
    "# Scale the data~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "def scale_data(X_train, X_validate, X_test):\n",
    "    '''\n",
    "    This function will scale numeric data using Min Max transform after \n",
    "    it has already been split into train, validate, and test.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    obj_col = ['open_date', 'due_date', 'closed_date', 'is_late', 'dept', 'call_reason', 'case_type', 'case_status', 'source_id', 'address', 'zipcode']\n",
    "    num_train = X_train.drop(columns = obj_col)\n",
    "    num_validate = X_validate.drop(columns = obj_col)\n",
    "    num_test = X_test.drop(columns = obj_col)\n",
    "    \n",
    "    \n",
    "    # Make the thing\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    \n",
    "   \n",
    "    # we only .fit on the training data\n",
    "    scaler.fit(num_train)\n",
    "    train_scaled = scaler.transform(num_train)\n",
    "    validate_scaled = scaler.transform(num_validate)\n",
    "    test_scaled = scaler.transform(num_test)\n",
    "    \n",
    "    # turn the numpy arrays into dataframes\n",
    "    train_scaled = pd.DataFrame(train_scaled, columns=num_train.columns)\n",
    "    validate_scaled = pd.DataFrame(validate_scaled, columns=num_train.columns)\n",
    "    test_scaled = pd.DataFrame(test_scaled, columns=num_train.columns)\n",
    "    \n",
    "    \n",
    "    return train_scaled, validate_scaled, test_scaled\n",
    "\n",
    "# Combo Train & Scale Function~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "def split_separate_scale(df, stratify_by= 'level_of_delay'):\n",
    "    '''\n",
    "    This function will take in a dataframe\n",
    "    separate the dataframe into train, validate, and test dataframes\n",
    "    separate the target variable from train, validate and test\n",
    "    then it will scale the numeric variables in train, validate, and test\n",
    "    finally it will return all dataframes individually\n",
    "    '''\n",
    "    \n",
    "    # split data into train, validate, test\n",
    "    train, validate, test = split(df, stratify_by= 'level_of_delay')\n",
    "    \n",
    "     # seperate target variable\n",
    "    X_train, y_train, X_validate, y_validate, X_test, y_test = separate_y(train, validate, test)\n",
    "    \n",
    "    \n",
    "    # scale numeric variable\n",
    "    train_scaled, validate_scaled, test_scaled = scale_data(X_train, X_validate, X_test)\n",
    "    \n",
    "    return train, validate, test, X_train, y_train, X_validate, y_validate, X_test, y_test, train_scaled, validate_scaled, test_scaled\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def get_sq_miles(df):\n",
    "    \"\"\"\n",
    "    This function will apply the square miles per district\n",
    "    to each district.\n",
    "    \"\"\"\n",
    "    # Creating a dictionary with square miles from city of San Antonio to convert to dataframe\n",
    "    sq_miles = {1: 26.00, 2: 59.81, 3: 116.15, 4: 65.21, 5: 22.24, 6: 38.44, 7: 32.82,\n",
    "                         8: 71.64, 9: 48.71, 10: 55.62}\n",
    "    # Converting to dataframe\n",
    "    sq_miles = pd.DataFrame(list(sq_miles.items()),columns = ['council_district','square_miles'])\n",
    "    #Merging with the original dataframe\n",
    "    df = df.merge(sq_miles, on = 'council_district', how ='left')\n",
    "    return df\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def extract_time(df):\n",
    "    '''\n",
    "    This function will take in a dataframe and return it with new features extracted from the open_date column\n",
    "    - open_month: which month the case was opened in\n",
    "    - open_year: which year the case was opened in\n",
    "    - open_week: which week the case was opened in\n",
    "    '''\n",
    "    \n",
    "    # extract month from open_date\n",
    "    df['open_month'] = df.open_date.dt.month\n",
    "    \n",
    "    # extract year from open_date\n",
    "    df['open_year'] = df.open_date.dt.year\n",
    "    \n",
    "    # extract week from open_date\n",
    "    df['open_week'] = df.open_date.dt.week\n",
    "    \n",
    "    return df\n",
    "    \n",
    "#------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def find_voter_info(df):\n",
    "    '''This function reads in a dataframe. Using the Council District column, it appends the voter turn out and\n",
    "    number of registered voters for each district. It does NOT take into account district 0 due to that being a\n",
    "    filler district for outside jurisdictions. It then appends the info onto the dataframe and returns it for later use.'''\n",
    "    conditions = [\n",
    "    (df['Council District'] == 1),\n",
    "    (df['Council District'] == 2), \n",
    "    (df['Council District'] == 3),\n",
    "    (df['Council District'] == 4),\n",
    "    (df['Council District'] == 5),\n",
    "    (df['Council District'] == 6),\n",
    "    (df['Council District'] == 7),\n",
    "    (df['Council District'] == 8),\n",
    "    (df['Council District'] == 9),\n",
    "    (df['Council District'] == 10)\n",
    "    ]\n",
    "    # create a list of the values we want to assign for each condition\n",
    "    voter_turnout = [0.148, 0.086, 0.111, 0.078, 0.085, 0.124, 0.154, 0.137, 0.185, 0.154]\n",
    "    registered_voters= [68081, 67656, 69022, 66370, 61418, 80007, 83287, 97717, 99309, 91378]\n",
    "    # create a new column and use np.select to assign values to it using our lists as arguments\n",
    "    df['voter_turnout_2019'] = np.select(conditions, voter_turnout)\n",
    "    df['num_of_registered_voters'] = np.select(conditions, registered_voters)\n",
    "    return df\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def add_per_cap_in(df):\n",
    "    '''\n",
    "    This function takes in the original cleaned dataframe and adds a per capita\n",
    "    income metric by council district\n",
    "    '''\n",
    "    # Creating a dictionary with per_capita values from city of San Antonio to convert to dataframe\n",
    "    per_cap_in = {1: 23967, 2: 19055, 3: 18281, 4: 18500, 5: 13836, 6: 23437, 7: 25263,\n",
    "                         8: 35475, 9: 42559, 10: 30240}\n",
    "    # Converting to dataframe\n",
    "    per_cap_in = pd.DataFrame(list(per_cap_in.items()),columns = ['council_district','per_capita_income'])\n",
    "    #Merging with the original dataframe\n",
    "    df = df.merge(per_cap_in, on = 'council_district', how ='left')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = split(df, stratify_by= 'level_of_delay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_validate, y_validate, X_test, y_test = separate_y(train, validate, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "def get_311_data():\n",
    "    '''\n",
    "    This function uses pandas read .csv to read in the downloaded .csv \n",
    "    from: https://data.sanantonio.gov/dataset/service-calls/resource/20eb6d22-7eac-425a-85c1-fdb365fd3cd7\n",
    "    after the .csv is read in, it returns it as a data frame.\n",
    "    '''\n",
    "    df= pd.read_csv('service_calls.csv')\n",
    "    return df\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Set the index\n",
    "def drop_and_index(df):\n",
    "    \"\"\"\n",
    "    This function will take in one positional argurment:\n",
    "    1.  311 df\n",
    "    This function will perform the following operations to the df:\n",
    "    1.  Drop category, drop report starting date, and drop report\n",
    "    ending date\n",
    "    2.  Set CASEID as the index\n",
    "    \"\"\"\n",
    "    # Drop category, report starting date, and report ending date\n",
    "    df.drop(columns=['Category', \n",
    "                 'Report Starting Date', \n",
    "                 'Report Ending Date' ], inplace=True)\n",
    "    # Set index to case id\n",
    "    df.set_index('CASEID', inplace=True)\n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Handle null values\n",
    "def handle_nulls(df):\n",
    "    '''This funciton takes in data frame\n",
    "    drops nulls for specified features\n",
    "    replaces nulls with \"Unknown\" for specific feature\n",
    "    removes observations where City Council is the Department'''\n",
    "    # drop null values\n",
    "    df.dropna(subset = ['SLA_Date', 'XCOORD', 'YCOORD'], inplace = True)\n",
    "    # replace null values\n",
    "    df.fillna({'Dept': 'Unknown'}, inplace = True)\n",
    "    # remove city council department\n",
    "    df = df[df.Dept != 'City Council']\n",
    "    # return df\n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Create delay columns\n",
    "def create_delay_columns(df):\n",
    "    '''This funciton takes in the dataframe\n",
    "    reformats specified columns into datetime format\n",
    "    create 2 columns to see time a case was open \n",
    "    and the time it was given for a resolution to be found\n",
    "    create another feature for how long it took \n",
    "        compare to due date for a resolution to be found\n",
    "    bin how long it took compared to due date\n",
    "    fill nulls with \"Still Open\"\n",
    "    return df\n",
    "    '''\n",
    "    # make sure the open and closed date columns are formatted in datetime format\n",
    "    df['OPENEDDATETIME'] = pd.to_datetime(df['OPENEDDATETIME'])\n",
    "    df['CLOSEDDATETIME'] = pd.to_datetime(df['CLOSEDDATETIME'])\n",
    "    df['SLA_Date'] = pd.to_datetime(df['SLA_Date'])\n",
    "    # create new number of days open feature\n",
    "    df['days_open'] = df['CLOSEDDATETIME'] - df['OPENEDDATETIME']\n",
    "    # Create new column to hold days before needed resolution\n",
    "    df['resolution_days_due'] = df['SLA_Date'] - df['OPENEDDATETIME']\n",
    "    # Convert to string format insted of timedelta\n",
    "    df['days_open'] = df.days_open // pd.Timedelta('1d')\n",
    "    # Convert to string format insted of timedelta\n",
    "    df['resolution_days_due'] = df.resolution_days_due // pd.Timedelta('1d')\n",
    "    # create new feature to show how long it took to resolve compared to resolution due date\n",
    "    df['days_before_or_after_due'] = df.resolution_days_due - df.days_open\n",
    "    # replace null values in days open with 0\n",
    "    df['days_open'] = df['days_open'].fillna(0)\n",
    "    # add 1 to resolution days to offset future issues with upcoming feature\n",
    "    df['resolution_days_due'] = df['resolution_days_due'] + 1\n",
    "    # create new feature to show how long it took to resolve compared to resolution due date\n",
    "    df['pct_time_of_used'] = df.days_open / df.resolution_days_due\n",
    "    # bin the new feature\n",
    "    df['level_of_delay'] = pd.cut(df.pct_time_of_used, \n",
    "                            bins = [0.0,0.25,0.5,0.75,1.0,15,100,200],\n",
    "                            labels = ['Extremely Early Response', 'Very Early Response', \n",
    "                                      'Early Response', \"On Time Response\", \"Late Response\", \n",
    "                                      'Very Late Response', 'Extremely Late Response'])\n",
    "    # drop nulls in these columns\n",
    "    df.dropna(subset=['days_open'], how='all', inplace=True)\n",
    "    df.dropna(subset=['level_of_delay'], how='all', inplace=True)\n",
    "    # return df\n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "def handle_outliers(df):\n",
    "    '''removes outiers from df'''\n",
    "    # remove outliers from days_open\n",
    "    df = df[df.days_open < 1400]\n",
    "    # return df\n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "def create_dummies(df):\n",
    "    '''This function creates dummy variables for Council Districts'''\n",
    "    # Drop district 0\n",
    "    df = df[df['Council District'] != 0]\n",
    "    # set what we are going to create these dummies from\n",
    "    dummy_df =  pd.get_dummies(df['Council District'])\n",
    "    # Name the new columns\n",
    "    dummy_df.columns = ['district_1', 'district_2', \n",
    "                        'district_3', 'district_4', 'district_5',\n",
    "                        'district_6', 'district_7', 'district_8',\n",
    "                        'district_9', 'district_10']\n",
    "    # add the dummies to the data frame\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "    return df\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "def clean_reason(df):\n",
    "    '''\n",
    "    This function will take in the service call df and replace the content of REASONNAME column with condensed names\n",
    "    '''\n",
    "    # replace with waste\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Waste Collection', 'Solid Waste', 'Brush'], 'waste')\n",
    "    # replace with code\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Code Enforcement', 'Code Enforcement (Internal)', 'Code Enforcement (IntExp)'], 'code')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Field Operations', 'Vector'], 'field')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace('Miscellaneous', 'misc')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Traffic Operations', 'Traffic Engineering Design', 'Traffic Issue Investigation'], 'traffic')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Streets', 'Signals', 'Signs and Markings'], 'streets')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace('Trades', 'trades')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Stormwater', 'Storm Water', 'Storm Water Engineering'], 'storm')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Small Business', 'Food Establishments', 'Shops (Internal)', 'Shops'], 'business')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace('Workforce Development', 'workforce')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Customer Service', '311 Call Center', 'Director\\'s Office Horizontal'], 'customer_service')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Land Development', 'Clean and Green', 'Urban Forestry', 'Natural Resources', 'Park Projects', 'Tree Crew', 'District 2', 'Clean and Green Natural Areas'], 'land')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace('Facility License', 'license')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Dangerous Premise','Historic Preservation', 'Engineering Division'], 'buildings')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Graffiti (IntExp)', 'General Sanitation', 'Graffiti'], 'cleanup')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Waste Collection', 'Solid Waste'], 'waste')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Waste Collection', 'Solid Waste'], 'waste')\n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# rename columns\n",
    "def clean_column_names(df):\n",
    "    '''This function reads in a dataframe as a positional argument, makes the column names easier to call and\n",
    "    more python friendly. It also extracts the zip code from the address column. It then returns a cleaned data \n",
    "    frame.'''\n",
    "    df= df.rename(columns={\n",
    "                    'Category':'category', 'OPENEDDATETIME':'open_date', 'Dept': 'dept',\n",
    "                    'SLA_Date':'due_date', 'CLOSEDDATETIME': 'closed_date', 'Late (Yes/No)': 'is_late',\n",
    "                    'OBJECTDESC': 'address', 'REASONNAME': 'call_reason', 'TYPENAME': 'case_type', \n",
    "                    'Council District': 'council_district', 'CASEID': 'case_id',\n",
    "                    'CaseStatus': 'case_status', 'SourceID':'source_id', 'XCOORD': 'longitude', 'YCOORD': 'latitude',\n",
    "                    'Report Starting Date': 'report_start_date', 'Report Ending Date': 'report_end_date'\n",
    "                      })\n",
    "    df['zipcode'] = df['address'].str.extract(r'.*(\\d{5}?)$')  \n",
    "    #drop zipcode nulls after obtaining zipcode\n",
    "    df.dropna(subset=['zipcode'], how='all', inplace=True)         \n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# clean the whole df\n",
    "def first_iteration_clean_311(df):\n",
    "    '''Takes in all previous funcitons to clean the whole df'''\n",
    "    # Drop columns and set index\n",
    "    df = drop_and_index(df)\n",
    "    # hadle null values\n",
    "    df = handle_nulls(df)\n",
    "    # creating delay involved columns\n",
    "    df = create_delay_columns(df)\n",
    "    # handle outliers\n",
    "    df = handle_outliers(df)\n",
    "    # make dummies\n",
    "    df = create_dummies(df)\n",
    "    # merge reasons\n",
    "    df = clean_reason(df)\n",
    "    # rename columns\n",
    "    df = clean_column_names(df)\n",
    "    df.to_csv('first_iteration_clean_311.csv')\n",
    "    # return df\n",
    "    return df\n",
    "\n",
    "\n",
    "# Train/Split the data~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "def split(df, stratify_by= 'level_of_delay'):\n",
    "    \"\"\"\n",
    "    Crude train, validate, test split\n",
    "    To stratify, send in a column name\n",
    "    \"\"\"\n",
    "    if stratify_by == None:\n",
    "        train, test = train_test_split(df, test_size=.2, random_state=319)\n",
    "        train, validate = train_test_split(train, test_size=.3, random_state=319)\n",
    "    else:\n",
    "        train, test = train_test_split(df, test_size=.2, random_state=319, stratify=df[stratify_by])\n",
    "        train, validate = train_test_split(train, test_size=.3, random_state=319, stratify=train[stratify_by])\n",
    "    return train, validate, test\n",
    "\n",
    "# Create X_train, y_train, etc...~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "def separate_y(train, validate, test):\n",
    "    '''\n",
    "    This function will take the train, validate, and test dataframes and separate the target variable into its\n",
    "    own panda series\n",
    "    '''\n",
    "    \n",
    "    X_train = train.drop(columns=['level_of_delay'])\n",
    "    y_train = train.level_of_delay\n",
    "    X_validate = validate.drop(columns=['level_of_delay'])\n",
    "    y_validate = validate.level_of_delay\n",
    "    X_test = test.drop(columns=['level_of_delay'])\n",
    "    y_test = test.level_of_delay\n",
    "    return X_train, y_train, X_validate, y_validate, X_test, y_test\n",
    "\n",
    "# Scale the data~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "def scale_data(X_train, X_validate, X_test):\n",
    "    '''\n",
    "    This function will scale numeric data using Min Max transform after \n",
    "    it has already been split into train, validate, and test.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    obj_col = ['open_date', 'due_date', 'closed_date', 'is_late', 'dept', 'call_reason', 'case_type', 'case_status', 'source_id', 'address', 'zipcode']\n",
    "    num_train = X_train.drop(columns = obj_col)\n",
    "    num_validate = X_validate.drop(columns = obj_col)\n",
    "    num_test = X_test.drop(columns = obj_col)\n",
    "    \n",
    "    \n",
    "    # Make the thing\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    \n",
    "   \n",
    "    # we only .fit on the training data\n",
    "    scaler.fit(num_train)\n",
    "    train_scaled = scaler.transform(num_train)\n",
    "    validate_scaled = scaler.transform(num_validate)\n",
    "    test_scaled = scaler.transform(num_test)\n",
    "    \n",
    "    # turn the numpy arrays into dataframes\n",
    "    train_scaled = pd.DataFrame(train_scaled, columns=num_train.columns)\n",
    "    validate_scaled = pd.DataFrame(validate_scaled, columns=num_train.columns)\n",
    "    test_scaled = pd.DataFrame(test_scaled, columns=num_train.columns)\n",
    "    \n",
    "    \n",
    "    return train_scaled, validate_scaled, test_scaled\n",
    "\n",
    "# Combo Train & Scale Function~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "def split_separate_scale(df, stratify_by= 'level_of_delay'):\n",
    "    '''\n",
    "    This function will take in a dataframe\n",
    "    separate the dataframe into train, validate, and test dataframes\n",
    "    separate the target variable from train, validate and test\n",
    "    then it will scale the numeric variables in train, validate, and test\n",
    "    finally it will return all dataframes individually\n",
    "    '''\n",
    "    \n",
    "    # split data into train, validate, test\n",
    "    train, validate, test = split(df, stratify_by= 'level_of_delay')\n",
    "    \n",
    "     # seperate target variable\n",
    "    X_train, y_train, X_validate, y_validate, X_test, y_test = separate_y(train, validate, test)\n",
    "    \n",
    "    \n",
    "    # scale numeric variable\n",
    "    train_scaled, validate_scaled, test_scaled = scale_data(X_train, X_validate, X_test)\n",
    "    \n",
    "    return train, validate, test, X_train, y_train, X_validate, y_validate, X_test, y_test, train_scaled, validate_scaled, test_scaled\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def get_sq_miles(df):\n",
    "    \"\"\"\n",
    "    This function will apply the square miles per district\n",
    "    to each district.\n",
    "    \"\"\"\n",
    "    # Creating a dictionary with square miles from city of San Antonio to convert to dataframe\n",
    "    sq_miles = {1: 26.00, 2: 59.81, 3: 116.15, 4: 65.21, 5: 22.24, 6: 38.44, 7: 32.82,\n",
    "                         8: 71.64, 9: 48.71, 10: 55.62}\n",
    "    # Converting to dataframe\n",
    "    sq_miles = pd.DataFrame(list(sq_miles.items()),columns = ['council_district','square_miles'])\n",
    "    #Merging with the original dataframe\n",
    "    df = df.merge(sq_miles, on = 'council_district', how ='left')\n",
    "    return df\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def extract_time(df):\n",
    "    '''\n",
    "    This function will take in a dataframe and return it with new features extracted from the open_date column\n",
    "    - open_month: which month the case was opened in\n",
    "    - open_year: which year the case was opened in\n",
    "    - open_week: which week the case was opened in\n",
    "    '''\n",
    "    \n",
    "    # extract month from open_date\n",
    "    df['open_month'] = df.open_date.dt.month\n",
    "    \n",
    "    # extract year from open_date\n",
    "    df['open_year'] = df.open_date.dt.year\n",
    "    \n",
    "    # extract week from open_date\n",
    "    df['open_week'] = df.open_date.dt.week\n",
    "    \n",
    "    return df\n",
    "    \n",
    "#------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def find_voter_info(df):\n",
    "    '''This function reads in a dataframe. Using the Council District column, it appends the voter turn out and\n",
    "    number of registered voters for each district. It does NOT take into account district 0 due to that being a\n",
    "    filler district for outside jurisdictions. It then appends the info onto the dataframe and returns it for later use.'''\n",
    "    conditions = [\n",
    "    (df['Council District'] == 1),\n",
    "    (df['Council District'] == 2), \n",
    "    (df['Council District'] == 3),\n",
    "    (df['Council District'] == 4),\n",
    "    (df['Council District'] == 5),\n",
    "    (df['Council District'] == 6),\n",
    "    (df['Council District'] == 7),\n",
    "    (df['Council District'] == 8),\n",
    "    (df['Council District'] == 9),\n",
    "    (df['Council District'] == 10)\n",
    "    ]\n",
    "    # create a list of the values we want to assign for each condition\n",
    "    voter_turnout = [0.148, 0.086, 0.111, 0.078, 0.085, 0.124, 0.154, 0.137, 0.185, 0.154]\n",
    "    registered_voters= [68081, 67656, 69022, 66370, 61418, 80007, 83287, 97717, 99309, 91378]\n",
    "    # create a new column and use np.select to assign values to it using our lists as arguments\n",
    "    df['voter_turnout_2019'] = np.select(conditions, voter_turnout)\n",
    "    df['num_of_registered_voters'] = np.select(conditions, registered_voters)\n",
    "    return df\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def add_per_cap_in(df):\n",
    "    '''\n",
    "    This function takes in the original cleaned dataframe and adds a per capita\n",
    "    income metric by council district\n",
    "    '''\n",
    "    # Creating a dictionary with per_capita values from city of San Antonio to convert to dataframe\n",
    "    per_cap_in = {1: 23967, 2: 19055, 3: 18281, 4: 18500, 5: 13836, 6: 23437, 7: 25263,\n",
    "                         8: 35475, 9: 42559, 10: 30240}\n",
    "    # Converting to dataframe\n",
    "    per_cap_in = pd.DataFrame(list(per_cap_in.items()),columns = ['council_district','per_capita_income'])\n",
    "    #Merging with the original dataframe\n",
    "    df = df.merge(per_cap_in, on = 'council_district', how ='left')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "def get_311_data():\n",
    "    '''\n",
    "    This function uses pandas read .csv to read in the downloaded .csv \n",
    "    from: https://data.sanantonio.gov/dataset/service-calls/resource/20eb6d22-7eac-425a-85c1-fdb365fd3cd7\n",
    "    after the .csv is read in, it returns it as a data frame.\n",
    "    '''\n",
    "    df= pd.read_csv('service_calls.csv')\n",
    "    return df\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Set the index\n",
    "def drop_and_index(df):\n",
    "    \"\"\"\n",
    "    This function will take in one positional argurment:\n",
    "    1.  311 df\n",
    "    This function will perform the following operations to the df:\n",
    "    1.  Drop category, drop report starting date, and drop report\n",
    "    ending date\n",
    "    2.  Set CASEID as the index\n",
    "    \"\"\"\n",
    "    # Drop category, report starting date, and report ending date\n",
    "    df.drop(columns=['Category', \n",
    "                 'Report Starting Date', \n",
    "                 'Report Ending Date' ], inplace=True)\n",
    "    # Set index to case id\n",
    "    df.set_index('CASEID', inplace=True)\n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Handle null values\n",
    "def handle_nulls(df):\n",
    "    '''This funciton takes in data frame\n",
    "    drops nulls for specified features\n",
    "    replaces nulls with \"Unknown\" for specific feature\n",
    "    removes observations where City Council is the Department'''\n",
    "    # drop null values\n",
    "    df.dropna(subset = ['SLA_Date', 'XCOORD', 'YCOORD'], inplace = True)\n",
    "    # replace null values\n",
    "    df.fillna({'Dept': 'Unknown'}, inplace = True)\n",
    "    # remove city council department\n",
    "    df = df[df.Dept != 'City Council']\n",
    "    # return df\n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Create delay columns\n",
    "def create_delay_columns(df):\n",
    "    '''This funciton takes in the dataframe\n",
    "    reformats specified columns into datetime format\n",
    "    create 2 columns to see time a case was open \n",
    "    and the time it was given for a resolution to be found\n",
    "    create another feature for how long it took \n",
    "        compare to due date for a resolution to be found\n",
    "    bin how long it took compared to due date\n",
    "    fill nulls with \"Still Open\"\n",
    "    return df\n",
    "    '''\n",
    "    # make sure the open and closed date columns are formatted in datetime format\n",
    "    df['OPENEDDATETIME'] = pd.to_datetime(df['OPENEDDATETIME'])\n",
    "    df['CLOSEDDATETIME'] = pd.to_datetime(df['CLOSEDDATETIME'])\n",
    "    df['SLA_Date'] = pd.to_datetime(df['SLA_Date'])\n",
    "    # create new number of days open feature\n",
    "    df['days_open'] = df['CLOSEDDATETIME'] - df['OPENEDDATETIME']\n",
    "    # Create new column to hold days before needed resolution\n",
    "    df['resolution_days_due'] = df['SLA_Date'] - df['OPENEDDATETIME']\n",
    "    # Convert to string format insted of timedelta\n",
    "    df['days_open'] = df.days_open // pd.Timedelta('1d')\n",
    "    # Convert to string format insted of timedelta\n",
    "    df['resolution_days_due'] = df.resolution_days_due // pd.Timedelta('1d')\n",
    "    # create new feature to show how long it took to resolve compared to resolution due date\n",
    "    df['days_before_or_after_due'] = df.resolution_days_due - df.days_open\n",
    "        # postitive means before days before due data and negative means number of days after due\n",
    "    # bin how long it took compare to due date to get level of delay\n",
    "    df['level_of_delay'] = pd.cut(df.days_before_or_after_due, \n",
    "                                bins = [-700,-500,-300,-100,0,100,300,500],\n",
    "                                labels = ['Extremely Late Response', 'Very Late Response', \n",
    "                                          'Late Response', \"On Time Response\", \"Early Response\", \n",
    "                                          'Very Early Response', 'Extremely Early Response'])\n",
    "    # drop nulls in these columns\n",
    "    print('before:', df.shape)\n",
    "    df.dropna(subset=['days_open'], how='all', inplace=True)\n",
    "    print('after:', df.shape)\n",
    "    print(df.isna().sum())\n",
    "    df.dropna(subset=['level_of_delay'], how='all', inplace=True)\n",
    "    print('after level of delay drop', df.shape)\n",
    "    print(df.isna().sum())\n",
    "    # return new df\n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "def handle_outliers(df):\n",
    "    '''removes outiers from df'''\n",
    "    # remove outliers from days_open\n",
    "    df = df[df.days_open < 1400]\n",
    "    # return df\n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "def create_dummies(df):\n",
    "    '''This function creates dummy variables for Council Districts'''\n",
    "    # set what we are going to create these dummies from\n",
    "    dummy_df =  pd.get_dummies(df['Council District'])\n",
    "    # Name the new columns\n",
    "    dummy_df.columns = ['district_0', 'district_1', 'district_2', \n",
    "                        'district_3', 'district_4', 'district_5',\n",
    "                        'district_6', 'district_7', 'district_8',\n",
    "                        'district_9', 'district_10']\n",
    "    # add the dummies to the data frame\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "def clean_reason(df):\n",
    "    '''\n",
    "    This function will take in the service call df and replace the content of REASONNAME column with condensed names\n",
    "    '''\n",
    "    # replace with waste\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Waste Collection', 'Solid Waste', 'Brush'], 'waste')\n",
    "    # replace with code\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Code Enforcement', 'Code Enforcement (Internal)', 'Code Enforcement (IntExp)'], 'code')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Field Operations', 'Vector'], 'field')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace('Miscellaneous', 'misc')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Traffic Operations', 'Traffic Engineering Design', 'Traffic Issue Investigation'], 'traffic')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Streets', 'Signals', 'Signs and Markings'], 'streets')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace('Trades', 'trades')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Stormwater', 'Storm Water', 'Storm Water Engineering'], 'storm')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Small Business', 'Food Establishments', 'Shops (Internal)', 'Shops'], 'business')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace('Workforce Development', 'workforce')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Customer Service', '311 Call Center', 'Director\\'s Office Horizontal'], 'customer_service')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Land Development', 'Clean and Green', 'Urban Forestry', 'Natural Resources', 'Park Projects', 'Tree Crew', 'District 2', 'Clean and Green Natural Areas'], 'land')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace('Facility License', 'license')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Dangerous Premise','Historic Preservation', 'Engineering Division'], 'buildings')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Graffiti (IntExp)', 'General Sanitation', 'Graffiti'], 'cleanup')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Waste Collection', 'Solid Waste'], 'waste')\n",
    "    # replace with\n",
    "    df['REASONNAME'] = df['REASONNAME'].replace(['Waste Collection', 'Solid Waste'], 'waste')\n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# rename columns\n",
    "def clean_column_names(df):\n",
    "    '''This function reads in a dataframe as a positional argument, makes the column names easier to call and\n",
    "    more python friendly. It also extracts the zip code from the address column. It then returns a cleaned data \n",
    "    frame.'''\n",
    "    df= df.rename(columns={\n",
    "                    'Category':'category', 'OPENEDDATETIME':'open_date', 'Dept': 'dept',\n",
    "                    'SLA_Date':'due_date', 'CLOSEDDATETIME': 'closed_date', 'Late (Yes/No)': 'is_late',\n",
    "                    'OBJECTDESC': 'address', 'REASONNAME': 'call_reason', 'TYPENAME': 'case_type', \n",
    "                    'Council District': 'council_district', 'CASEID': 'case_id',\n",
    "                    'CaseStatus': 'case_status', 'SourceID':'source_id', 'XCOORD': 'longitude', 'YCOORD': 'latitude',\n",
    "                    'Report Starting Date': 'report_start_date', 'Report Ending Date': 'report_end_date'\n",
    "                      })\n",
    "    df['zipcode'] = df['address'].str.extract(r'.*(\\d{5}?)$')  \n",
    "    #drop zipcode nulls after obtaining zipcode\n",
    "    df.dropna(subset=['zipcode'], how='all', inplace=True)         \n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# clean the whole df\n",
    "def clean_311(df):\n",
    "    '''Takes in all previous funcitons to clean the whole df'''\n",
    "    # Drop columns and set index\n",
    "    df = drop_and_index(df)\n",
    "    print('drop and index', df.shape)\n",
    "    # hadle null values\n",
    "    df = handle_nulls(df)\n",
    "    print('handle nulls', df.shape)\n",
    "    # creating delay involved columns\n",
    "    df = create_delay_columns(df)\n",
    "    print('delay columns', df.shape)\n",
    "    # handle outliers\n",
    "    df = handle_outliers(df)\n",
    "    print('outliers', df.shape)\n",
    "    # make dummies\n",
    "    df = create_dummies(df)\n",
    "    print('dummies', df.shape)\n",
    "    # merge reasons\n",
    "    df = clean_reason(df)\n",
    "    print('clean reason', df.shape)\n",
    "    # rename columns\n",
    "    df = clean_column_names(df)\n",
    "    print('clean column names', df.shape)\n",
    "    #df.to_csv('clean_311.csv')\n",
    "    # return df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop and index (495440, 13)\n",
      "handle nulls (481205, 13)\n",
      "before: (481205, 17)\n",
      "after: (431696, 17)\n",
      "OPENEDDATETIME                 0\n",
      "SLA_Date                       0\n",
      "CLOSEDDATETIME                 0\n",
      "Late (Yes/No)                  0\n",
      "Dept                           0\n",
      "REASONNAME                     0\n",
      "TYPENAME                       0\n",
      "CaseStatus                     0\n",
      "SourceID                       0\n",
      "OBJECTDESC                     0\n",
      "Council District               0\n",
      "XCOORD                         0\n",
      "YCOORD                         0\n",
      "days_open                      0\n",
      "resolution_days_due            0\n",
      "days_before_or_after_due       0\n",
      "level_of_delay              2825\n",
      "dtype: int64\n",
      "after level of delay drop (428871, 17)\n",
      "OPENEDDATETIME              0\n",
      "SLA_Date                    0\n",
      "CLOSEDDATETIME              0\n",
      "Late (Yes/No)               0\n",
      "Dept                        0\n",
      "REASONNAME                  0\n",
      "TYPENAME                    0\n",
      "CaseStatus                  0\n",
      "SourceID                    0\n",
      "OBJECTDESC                  0\n",
      "Council District            0\n",
      "XCOORD                      0\n",
      "YCOORD                      0\n",
      "days_open                   0\n",
      "resolution_days_due         0\n",
      "days_before_or_after_due    0\n",
      "level_of_delay              0\n",
      "dtype: int64\n",
      "delay columns (428871, 17)\n",
      "outliers (428870, 17)\n",
      "dummies (428870, 28)\n",
      "clean reason (428870, 28)\n",
      "clean column names (356715, 29)\n"
     ]
    }
   ],
   "source": [
    "df = clean_311(get_311_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.council_district != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['council_district'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5     49986\n",
       "2     46987\n",
       "1     45889\n",
       "3     45709\n",
       "4     34595\n",
       "7     33447\n",
       "6     30364\n",
       "10    29372\n",
       "9     19437\n",
       "8     19435\n",
       "Name: council_district, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.council_district.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(355221, 29)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
